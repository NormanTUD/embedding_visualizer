<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Interactive Tokenizer → Embedding Trainer (didactic)</title>

  <script src="./tf.min.js"></script>
  <script src="./plotly.latest.js"></script>

  <style>
    :root {
      --bg: #071325;
      --panel: rgba(255,255,255,0.03);
      --muted: #9aa4b2;
      --accent: #5eead4;
      --card-shadow: 0 8px 24px rgba(2,6,23,0.6);
      --radius: 12px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
    }
    html,body { height:100%; margin:0; font-family:Inter,system-ui, -apple-system, "Segoe UI", Roboto, Arial; background: linear-gradient(180deg,#04121b,#071325); color:#e6eef6; -webkit-font-smoothing:antialiased; }
    .container { max-width:1200px; margin:20px auto 120px; padding:16px; display:grid; grid-template-columns: 1fr 420px; gap:16px; }
    header { display:flex; align-items:center; justify-content:space-between; gap:12px; margin-bottom:12px; }
    .brand { display:flex; gap:12px; align-items:center; }
    .logo { width:48px;height:48px;border-radius:10px;background:linear-gradient(135deg,var(--accent),#60a5fa);display:flex;align-items:center;justify-content:center;font-weight:700;color:#04293a; box-shadow: 0 6px 18px rgba(0,0,0,0.5); }
    h1 { font-size:18px; margin:0; }
    p.lead { margin:0; font-size:12px; color:var(--muted); }
    .panel { background:var(--panel); border-radius:var(--radius); box-shadow:var(--card-shadow); padding:12px; overflow:hidden; }
    .panel.small { padding:8px; }
    label { display:block; font-size:12px; color:var(--muted); margin-bottom:6px; }
    textarea, input, select { width:100%; padding:8px 10px; border-radius:8px; border:1px solid rgba(255,255,255,0.04); background:transparent; color:inherit; font-size:13px; box-sizing:border-box; outline:none; resize:vertical; min-height:40px; }
    .grid { display:grid; gap:8px; }
    .controls { display:flex; gap:8px; flex-wrap:wrap; }
    button { background:linear-gradient(180deg,#60a5fa,var(--accent)); color:#04293a; border:none; padding:8px 12px; border-radius:8px; cursor:pointer; font-weight:600; box-shadow: 0 6px 16px rgba(5,9,20,0.6); }
    button.ghost { background:transparent;color:var(--accent); border:1px solid rgba(255,255,255,0.04); }
    .token-output { display:flex; flex-wrap:wrap; gap:6px; }
    .token-pill { padding:6px 10px; border-radius:999px; background:rgba(255,255,255,0.02); font-family:var(--mono); font-size:12px; border:1px solid rgba(255,255,255,0.02); }
    #plot, #highdim-plot, #live-plot, .heatmap, #dist-heatmap { height:360px; min-height:220px; border-radius:10px; overflow:hidden; background:transparent; border:1px solid rgba(255,255,255,0.03); }
    #live-plot, #loss-plot { height:220px; }
    .status-bar { position:fixed; left:0;right:0;bottom:0; display:flex; align-items:center; justify-content:space-between; padding:10px 18px; background:linear-gradient(180deg, rgba(2,6,23,0.9), rgba(2,6,23,0.98)); color:var(--muted); border-top:1px solid rgba(255,255,255,0.02); font-size:13px; z-index:9999; }
    .logpane { max-height:200px; overflow:auto; background:rgba(0,0,0,0.06); padding:8px; border-radius:8px; font-family:var(--mono); font-size:12px; color:#cfe; }
    pre.mathml { background: rgba(0,0,0,0.06); padding:8px; border-radius:8px; overflow:auto; font-family:var(--mono); font-size:12px; color:#dfe; max-height:180px; }
    .explain { font-size:13px; color:#dfe; line-height:1.45; }
    .badge { font-family:var(--mono); font-size:12px; padding:4px 8px; border-radius:999px; background:rgba(255,255,255,0.02); }
    @media (max-width: 980px) { .container { grid-template-columns: 1fr; padding-bottom:180px; } #plot { height:320px; } #highdim-plot { height:260px; } #live-plot { height:200px; } }
  </style>
</head>
<body>
  <header class="container" style="max-width:1200px;">
    <div class="brand">
      <div class="logo">EMB</div>
      <div>
        <h1>Interactive Tokenizer & Embedding Trainer — Didactic</h1>
        <p class="lead">Ziel: jeder, der ungefähr weiß, wie ein neuronales Netz funktioniert, soll verstehen, was Encodings bedeuten.</p>
      </div>
    </div>
    <div style="display:flex;gap:8px;align-items:center;">
      <div class="badge" id="ui-status">idle</div>
      <button id="run-tests">Run tests</button>
    </div>
  </header>

  <main class="container" aria-live="polite">
    <section class="panel">
      <div style="display:flex;justify-content:space-between;align-items:center;">
        <strong>Eingabe & Tokenizer</strong>
        <small class="muted">Live-Tokenisierung</small>
      </div>

      <div class="grid" style="margin-top:8px;">
        <div>
          <label for="input-text">Haupttext</label>
          <textarea id="input-text" rows="6" placeholder="Gib Text ein..."></textarea>
        </div>

        <div style="display:flex;gap:8px;align-items:flex-end;">
          <div style="flex:1;">
            <label for="tokenizer-type">Tokenizer</label>
            <select id="tokenizer-type"><option value="whitespace">Whitespace</option><option value="comma">Comma</option><option value="regex">Regex</option><option value="ngram" selected>n-gram</option><option value="char">char</option></select>
          </div>
          <div style="width:140px;">
            <label for="ngram-n">n</label>
            <input id="ngram-n" type="number" min="1" value="2" />
          </div>
          <div style="width:160px;">
            <label for="ngram-mode">n-gram Mode</label>
            <select id="ngram-mode"><option value="word" selected>word n-grams</option><option value="char">character n-grams</option></select>
          </div>
        </div>

        <div>
          <label>Token-Vorschau</label>
          <div class="token-output" id="token-preview"></div>
        </div>

        <div style="display:flex;gap:8px;align-items:center;justify-content:space-between;">
          <div class="muted">Tokens: <strong id="stat-token-count">0</strong> • Unique: <strong id="stat-unique-count">0</strong></div>
          <div style="display:flex;gap:6px;">
            <button id="btn-tokenize" class="ghost">Tokenize</button>
            <button id="btn-build-vocab">Build vocab</button>
            <button id="btn-clear" class="ghost">Clear</button>
          </div>
        </div>
      </div>
    </section>

    <aside class="sidebar">
      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center;">
          <strong>Embedding & Training</strong>
          <small class="muted">Konfiguration</small>
        </div>
        <div class="grid" style="margin-top:8px;">
          <label for="embedding-dim">Embedding Länge (Dim)</label>
          <input id="embedding-dim" type="number" min="1" value="8" />
          <label for="visual-dims">Visual dims (1-3 zeigen im Hauptplot; >3 nutzt High-Dim Ansicht)</label>
          <select id="visual-dims"><option value="1">1</option><option value="2">2</option><option value="3">3</option><option value="4">4+</option></select>
          <label for="visual-mode">High-Dim Visualisierung</label>
          <select id="visual-mode">
            <option value="auto" selected>auto (parallel coords + pca + heatmap)</option>
            <option value="parallel">parallel coordinates</option>
            <option value="pca">PCA 2D</option>
            <option value="pairwise">Pairwise scatter matrix</option>
            <option value="heatmap">Correlation heatmap</option>
            <option value="tsne">t-SNE/UMAP Projektion (didaktischer Placeholder)</option>
            <option value="dendrogram">Hierarchisches Dendrogramm (didaktischer Placeholder)</option>
          </select>
          <label for="learning-rate">Learning rate</label>
          <input id="learning-rate" type="number" min="1e-6" step="1e-4" value="0.01" />
          <label for="optimizer-type">Optimizer</label>
          <select id="optimizer-type"><option value="sgd">SGD</option><option value="adam" selected>Adam</option></select>
          <label for="batch-size">Batch size</label>
          <input id="batch-size" type="number" min="1" value="8" />
          <label for="epochs">Epochs</label>
          <input id="epochs" type="number" min="1" value="6" />
          <div style="display:flex;gap:8px;">
            <button id="btn-create-model">Create model</button>
            <button id="btn-train">Train</button>
          </div>
          <div>
            <label>Model summary</label>
            <pre id="model-summary" style="font-family:var(--mono);font-size:12px;white-space:pre-wrap;max-height:140px;overflow:auto;"></pre>
          </div>
        </div>
      </div>
      
      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center;">
            <strong>Interaktive Embedding Analyse (4/10)</strong>
            <small class="muted">Nr. 6, 8, 9, 7 (unten)</small>
        </div>
        <div class="grid" style="margin-top:8px;">
            <div style="margin-bottom:10px;">
                <div class="explain">
                    <p>Diese Tools nutzen die Embeddings für tiefergehende didaktische Analysen, wie z.B. Vektor-Interpolation (Algebra) oder Ähnlichkeitsmuster im Text (Recurrence Plot).</p>
                </div>
            </div>

            <label for="radar-tokens">Token Profile (Radar Diagramm)</label>
            <input id="radar-tokens" placeholder="Gib Token mit Komma getrennt ein (z.B. Test,das,ist)" />
            <div id="radar-plot" style="height:250px; margin-top:8px;"></div>
            <button id="btn-plot-radar" class="ghost">Plot Profile (Nr. 6)</button>
            <hr style="border:none;border-top:1px solid rgba(255,255,255,0.05);margin:10px 0;">

            <label>Wort-Algebra (Vektor-Interpolation)</label>
            <div style="display:grid;grid-template-columns:1fr auto 1fr; gap:8px;align-items:center;">
                <input id="algebra-token-a" placeholder="Token A" value="mann" />
                <span class="muted">→</span>
                <input id="algebra-token-b" placeholder="Token B" value="frau" />
            </div>
            <div style="margin-top:8px;">
                <label for="algebra-slider">Interpolationsfaktor α: <span id="algebra-alpha">0.5</span></label>
                <input type="range" min="0" max="1" step="0.05" value="0.5" id="algebra-slider" style="height:8px;padding:0;"/>
            </div>
            <div id="algebra-result" class="explain" style="background:rgba(255,255,255,0.02);padding:8px;border-radius:8px;margin-top:8px;">
                **Result (A + α(B-A)):** <span id="algebra-result-token">Modell fehlt.</span>
            </div>
            <hr style="border:none;border-top:1px solid rgba(255,255,255,0.05);margin:10px 0;">

            <label>Recurrence Plot (Sequentielle Ähnlichkeit)</label>
            <div id="recurrence-plot" style="height:250px; margin-top:8px;"></div>
            <button id="btn-plot-recurrence" class="ghost">Plot Recurrence (Nr. 9)</button>
        </div>
      </div>
      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center;">
          <strong>Live während Training</strong>
          <small class="muted">separate Ansicht</small>
        </div>
        <div style="margin-top:8px;">
          <label for="live-text">Live Text</label>
          <input id="live-text" placeholder="Text zur Live-Visualisierung..." />
          <div style="display:flex;gap:8px;margin-top:8px;">
            <button id="btn-live-toggle" class="ghost">Live (off)</button>
            <button id="btn-probe-now" class="ghost">Probe Now</button>
          </div>
          <div id="live-plot" style="margin-top:8px;"></div>
          <div id="loss-plot" style="margin-top:8px; height:150px;"></div>
        </div>
      </div>

      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center;">
          <strong>Internals & Tensoren</strong>
          <small class="muted">MathML & Heatmaps</small>
        </div>
        <div style="margin-top:8px;">
          <div class="explain" style="margin-bottom:8px;">
            <strong>Was du hier siehst</strong>
            <p>Wähle "Show embedding matrix" oder "Show model internals". Matrizen werden <em>als gerenderte MathML</em> angezeigt (falls der Browser MathML unterstützt) und zusätzlich als Heatmap, damit du Muster wie ähnliche Vektoren oder starke/negative Werte visuell erkennen kannst.</p>
          </div>
          <div style="display:flex;gap:8px;">
            <button id="btn-show-emb-math" class="ghost">Show embedding matrix</button>
            <button id="btn-show-model-internals" class="ghost">Show model internals</button>
          </div>
          <div style="margin-top:8px;">
            <div id="tensor-meta" class="muted"></div>
            <div id="tensor-mathml-render" style="margin-top:8px;"></div>
            <div id="tensor-heatmap" class="heatmap" style="margin-top:8px;"></div>
            <div id="model-internals-render" style="margin-top:8px;"></div>
          </div>
        </div>
      </div>

      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center;">
          <strong>Didaktische Erklärungen</strong>
          <small class="muted">Warum Encodings wichtig sind</small>
        </div>
        <div style="margin-top:8px;" class="explain">
          <ol>
            <li><strong>Was ist ein Encoding?</strong> — Ein Encoding (Embedding) ist ein Vektor, der ein Token (Wort/Char/NG-gram) in einem kontinuierlichen Raum darstellt. Ähnliche Token haben ähnliche Vektoren.</li>
            <li><strong>Warum Vektoren?</strong> — Vektoren erlauben algebraische Operationen (z.B. Ähnlichkeit, Mittelwert), die semantische Beziehungen sichtbar machen.</li>
            <li><strong>Was zeigt die Matrix?</strong> — Jede Zeile der Embedding-Matrix ist das Encoding für ein Vocabulary-Element. Die Heatmap zeigt Muster: dunkle/helle Bereiche signalisieren hohe/geringe Werte; ähnliche Zeilen deuten auf ähnliche Bedeutungen.</li>
            <li><strong>Was sind Kernel / Gewichte?</strong> — Kernel in Dense/Conv-Schichten sind die Transformationen, die aus Encodings Vorhersagen machen. Visualisierte Kernel helfen zu verstehen, welche Dimensionskombinationen wichtig sind.</li>
            <li><strong>Interpretiere die Grafiken:</strong> Parallel Coordinates zeigen Profilmuster über viele Dimensionen. PCA/Pairwise macht hohe Dimmensionen sichtbar, indem ähnliche Punkte zusammengezogen werden. Heatmaps zeigen Korrelationen.</li>
          </ol>
          <p>Am Ende solltest du erkennen: Encodings sind numerische Repräsentationen — die Visualisierungen helfen zu sehen, welche Token nahe beieinander liegen und welche Dimensionsachsen (Features) stark variieren.</p>
        </div>
      </div>

      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center;">
          <strong>Logs</strong>
          <small class="muted">Konsole</small>
        </div>
        <div class="logpane" id="logpane" style="margin-top:8px;"></div>
      </div>
    </aside>

    <section class="panel" style="grid-column: 1 / -1;">
      <div style="display:flex;justify-content:space-between;align-items:center;">
        <strong>Haupt-Visualisierung</strong>
        <small class="muted">1/2/3D im Hauptplot — >3D in separatem High-Dim Bereich</small>
      </div>
      <div style="display:grid;grid-template-columns: 1fr 420px; gap:12px; margin-top:12px;">
        <div>
          <div id="plot"></div>
          <div style="display:flex;gap:8px;justify-content:space-between;align-items:center;margin-top:8px;">
            <div class="muted">Hauptplot — transparenter Hintergrund</div>
            <div style="display:flex;gap:6px;">
              <label><input type="checkbox" id="auto-highdim-toggle" checked> Auto High-Dim</label>
              <button id="btn-export-emb" class="ghost">Export embeddings</button>
              <button id="btn-reset" class="ghost">Reset model</button>
            </div>
          </div>
        </div>

        <div>
          <div style="display:flex;justify-content:space-between;align-items:center;">
            <strong>High-Dim Ansicht</strong>
            <small class="muted">Parallel coords / PCA / Pairwise / Heatmap</small>
          </div>
          <div id="highdim-plot" style="margin-top:8px;"></div>
        </div>
      </div>

      <div style="margin-top:16px;">
        <strong>Distanz/Ähnlichkeits-Heatmap</strong>
        <div id="dist-heatmap" style="margin-top:8px; height:360px;"></div>
      </div>
      
      <div style="margin-top:16px;">
          <strong>Histogramm-Verteilung der Embedding-Dimensionen (Nr. 7)</strong>
          <div id="dim-hist-plot" style="margin-top:8px; height:360px;"></div>
      </div>
      </section>
  </main>

  <div class="status-bar" role="status" aria-live="polite">
    <div id="status-last" class="muted">idle</div>
    <div class="muted" id="status-meta">logs: <span id="log-count">0</span> | emb size: <span id="status-emb-size">0</span></div>
  </div>

  <script>
    "use strict";
    ; 

    function isSymmetric(A, tolerance = 1e-6) {
	    // Überprüft, ob die tf.Tensor2D A symmetrisch ist (A = A^T) innerhalb einer gewissen Toleranz.
	    if (A.rank !== 2 || A.shape[0] !== A.shape[1]) {
		    return false;
	    }
	    let A_T = null;
	    let diff = null;
	    let maxAbsDiff = null;
	    try {
		    A_T = A.transpose();
		    diff = A.sub(A_T);
		    maxAbsDiff = tf.max(tf.abs(diff)).dataSync()[0];
		    return maxAbsDiff <= tolerance;
	    } catch (e) {
		    console.error("Fehler bei der Überprüfung der Symmetrie:", e);
		    return false;
	    } finally {
		    if (A_T) A_T.dispose();
		    if (diff) diff.dispose();
	    }
    }

    function eigh(A) {
	    // Eigendecomposition (Eigenwertzerlegung) für eine symmetrische Matrix A (Kovarianzmatrix)
	    if (!isSymmetric(A) || A.shape[0] !== A.shape[1]) {
		    throw new Error("eigh: Die Eingabematrix muss symmetrisch und quadratisch sein.");
	    }
	    const N = A.shape[0];
	    const MAX_ITERATIONS = 100;
	    const TOLERANCE = 1e-6;
	    const eigenvalues = [];
	    const eigenvectors = [];
	    let currentA = A.clone();

	    for (let k = 0; k < N; k++) {
		    let vk = tf.randomNormal([N, 1]);
		    let lambda = 0;
		    vk = vk.div(vk.norm());
		    for (let i = 0; i < MAX_ITERATIONS; i++) {
			    const Avk = currentA.matMul(vk);
			    const Avk_norm = Avk.norm();
			    const vk_new = Avk.div(Avk_norm);
			    const vk_T = vk_new.transpose();
			    const lambda_new_tensor = vk_T.matMul(currentA).matMul(vk_new);
			    const lambda_new = lambda_new_tensor.dataSync()[0];
			    const diff = Math.abs(lambda_new - lambda);
			    Avk.dispose();
			    vk_T.dispose();
			    lambda_new_tensor.dispose();
			    vk.dispose();
			    vk = vk_new;
			    lambda = lambda_new;
			    if (diff < TOLERANCE) {
				    break;
			    }
		    }
		    eigenvalues.push(lambda);
		    eigenvectors.push(vk.transpose());

		    if (k < N - 1) {
			    const v_vT = vk.matMul(vk.transpose());
			    const lambda_v_vT = v_vT.mul(lambda);
			    const nextA = currentA.sub(lambda_v_vT);
			    currentA.dispose();
			    v_vT.dispose();
			    lambda_v_vT.dispose();
			    currentA = nextA;
		    } else {
			    currentA.dispose();
		    }
		    vk.dispose();
	    }

	    const sortedPairs = eigenvalues.map((val, idx) => ({
		    value: val,
		    vector: eigenvectors[idx]
	    })).sort((a, b) => b.value - a.value);

	    const finalEigenvalues = tf.tensor1d(sortedPairs.map(p => p.value));
	    const finalEigenvectors = tf.concat(sortedPairs.map(p => p.vector), 0).transpose();

	    eigenvectors.forEach(t => t.dispose());

	    return { eigenvalues: finalEigenvalues, eigenvectors: finalEigenvectors };
    }

    function svd(centered) {
        let cov = null;
        let eig = null;
        try {
          const N = centered.shape[0];
          cov = centered.transpose().matMul(centered).div(N - 1 || 1);
          eig = eigh(cov);
          const v = eig.eigenvectors;
          const u = tf.zeros([centered.shape[0], centered.shape[1]]);
          eig.eigenvalues.dispose();
          cov.dispose();
          return { u, v };
        } catch(e) {
          console.error("Custom svd (via Eigen-decomposition) failed.", e);
          throw e;
        }
      }

    (function () {
      // Utilities
      const $ = s => document.querySelector(s);
      const $$ = s => Array.from(document.querySelectorAll(s));
      let logBuffer = [];
      let epochLosses = []; 
      let liveMode = false;

      function pushLog(level, message) {
        const entry = { time: new Date().toISOString(), level, message: String(message) };
        logBuffer.push(entry);
        if (logBuffer.length > 1000) logBuffer.shift();
        renderLogs();
        try { $("#status-last").textContent = `[${level}] ${message.split("\\n")[0]}`; $("#ui-status").textContent = level; } catch (e) {}
      }
      function renderLogs() {
        const el = $("#logpane");
        if (!el) return;
        el.innerHTML = logBuffer.slice(-80).map(e => `<div><small style="color:var(--muted)">[${e.time.split("T")[1].substring(0,8)}]</small> <strong>${e.level}</strong> ${escapeHtml(e.message)}</div>`).join("");
        el.scrollTop = el.scrollHeight;
        $("#log-count").textContent = logBuffer.length;
      }
      function escapeHtml(s) { return String(s).replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;"); }

      // Capture console
      (function () {
        const methods = ["log","info","warn","error"];
        methods.forEach(m => {
          const orig = console[m].bind(console);
          console[m] = function(...args) {
            try { pushLog(m, args.map(a => (a && a.stack) ? a.stack : (typeof a === "object" ? JSON.stringify(a) : String(a))).join(" ")); } catch (e) {}
            orig(...args);
          };
        });
      })();

      // Tokenizers
      const Tokenizers = (function () {
        function whitespaceTokenizer(text) { if (!text) return []; return text.split(/\s+/).filter(Boolean); }
        function commaTokenizer(text) { if (!text) return []; return text.split(",").map(s => s.trim()).filter(Boolean); }
        function regexTokenizer(text, opts) { if (!text) return []; const pattern = (opts && opts.pattern) || "\\b\\w+\\b"; try { const re = new RegExp(pattern,"g"); return text.match(re) || []; } catch (e) { console.warn("regex invalid", e); return []; } }
        function ngramTokenizer(text, opts) {
          if (!text) return [];
          const n = Math.max(1, (opts && parseInt(opts.n,10)) || 2);
          const mode = (opts && opts.mode) || "word";
          if (mode === "char") {
            const letters = Array.from(text.replace(/\s+/g,""));
            const out = [];
            for (let i=0;i<=letters.length-n;i++) out.push(letters.slice(i,i+n).join(""));
            return out;
          } else {
            const words = text.split(/\s+/).filter(Boolean);
            const out = [];
            for (let i=0;i<=words.length-n;i++) out.push(words.slice(i,i+n).join("_"));
            return out;
          }
        }
        function charTokenizer(text) { if (!text) return []; return Array.from(text.replace(/\s+/g,"").trim()); } 
        function buildTokenizer(type, opts) {
          switch(type) {
            case "whitespace": return t => whitespaceTokenizer(t,opts);
            case "comma": return t => commaTokenizer(t,opts);
            case "regex": return t => regexTokenizer(t,opts);
            case "ngram": return t => ngramTokenizer(t,opts);
            case "char": return t => charTokenizer(t,opts);
            default: return t => whitespaceTokenizer(t,opts);
          }
        }
        return { buildTokenizer, ngramTokenizer };
      })();

      // Vocab
      const Vocab = (function() {
        let tokenToId = {};
        let idToToken = [];
        function buildFromTokens(tokens) {
          tokenToId = {}; idToToken = [];
          tokens.forEach(t => { if (t && !(t in tokenToId)) { tokenToId[t] = idToToken.length; idToToken.push(t); } });
          console.info("vocab built", idToToken.length);
          return { tokenToId, idToToken };
        }
        function addTokens(tokens) { tokens.forEach(t => { if (t && !(t in tokenToId)) { tokenToId[t] = idToToken.length; idToToken.push(t); } }); }
        function size() { return idToToken.length; }
        function encode(tokens) { return tokens.map(t => { if (t in tokenToId) return tokenToId[t]; console.warn(`Token '${t}' not in vocab, adding it.`); tokenToId[t] = idToToken.length; idToToken.push(t); return tokenToId[t]; }).filter(i=>i>=0); } 
        function decode(ids) { return ids.map(i => idToToken[i] || "<UNK>"); }
        function reset() { tokenToId = {}; idToToken = []; }
        function exportVocab() { return { tokenToId: Object.assign({}, tokenToId), idToToken: idToToken.slice() }; }
        return { buildFromTokens, addTokens, size, encode, decode, reset, exportVocab };
      })();

      // Trainer
      const Trainer = (function() {
        let model = null; let created=false; let lastEmbedding=null;
        async function createModel(params) {
          try {
            if (model) resetModel();
            const inputDim = parseInt(params.inputDim,10);
            const embeddingDim = parseInt(params.outputDim || params.embeddingDim,10);
            if (!Number.isInteger(inputDim) || inputDim<=0) throw new Error("invalid inputDim");
            if (!Number.isInteger(embeddingDim) || embeddingDim<=0) throw new Error("invalid embeddingDim");
            const input = tf.input({shape:[1], dtype:"int32", name:"token_input"});
            const emb = tf.layers.embedding({ 
                inputDim: inputDim, outputDim: embeddingDim, inputLength: 1, name: "embedding", 
                embeddingsInitializer: 'glorotUniform'
            });
            const embedOut = emb.apply(input);
            const flat = tf.layers.flatten().apply(embedOut);
            const decoder = tf.layers.dense({ units: inputDim, activation: "softmax", name: "decoder" }).apply(flat);
            const m = tf.model({ inputs: input, outputs: decoder, name:"embed_model" });
            const opt = (params.optimizer === "sgd") ? tf.train.sgd(params.learningRate || 0.01) : tf.train.adam(params.learningRate || 0.01);
            m.compile({ optimizer: opt, loss: "sparseCategoricalCrossentropy", metrics: ["accuracy"] });
            model = m; created=true;
            try { let s=""; m.summary(80,{print: (l)=> s+=l+"\n" }); $("#model-summary").textContent = s; } catch(e) {}
            return m;
          } catch (err) { console.error("createModel", err); throw err; }
        }
        function hasModel(){ return created && model; }
        function getEmbeddingMatrix() {
          if (!hasModel()) return null;
          try { const layer = model.getLayer("embedding"); const weights = layer.getWeights(); if (weights && weights.length>0) { lastEmbedding = weights[0]; return lastEmbedding; } return null; } catch (e) { console.error(e); return null; }
        }
        async function trainOnDataset(encodedTokens, params, hooks) {
          if (!hasModel()) throw new Error("no model");
          if (!Array.isArray(encodedTokens) || encodedTokens.length===0) return;
          const N = encodedTokens.length;
          const xsArr = encodedTokens.map(v => [parseInt(v,10)]);
          const ysArr = encodedTokens.map(v => parseInt(v,10));
          
          const xs = tf.tensor2d(xsArr,[N,1],"int32"); 
          const ys = tf.tensor1d(ysArr,"float32"); 
          
          await model.fit(xs, ys, {
            batchSize: params.batchSize || 8,
            epochs: params.epochs || 1,
            shuffle: true,
            callbacks: {
              onBatchEnd: async (batch, logs) => { 
                await tf.nextFrame();
                if (hooks && hooks.onBatchEnd) await hooks.onBatchEnd(batch, logs); 
              },
              onEpochEnd: async (epoch, logs) => { 
                epochLosses.push(logs.loss);
                Visual.plotLoss(epochLosses);
                if (hooks && hooks.onEpochEnd) await hooks.onEpochEnd(epoch, logs); 
              }
            }
          });
          xs.dispose(); ys.dispose();
        }
        function resetModel(){ try { if (model) { model.dispose(); model=null; created=false; lastEmbedding=null; $("#model-summary").textContent=""; epochLosses=[]; Visual.plotLoss(epochLosses); } } catch(e){} }
        async function readModelInternals() {
          if (!hasModel()) return null;
          const layers = model.layers || [];
          const out = [];
          for (let i=0;i<layers.length;i++){
            const layer = layers[i];
            const weightTensors = layer.getWeights ? layer.getWeights() : [];
            const weightObjects = (layer.weights || []).slice();
            const ws = [];
            for (let j=0;j<weightTensors.length;j++){
              const wt = weightTensors[j];
              const name = (weightObjects[j] && weightObjects[j].name) ? weightObjects[j].name : `w_${j}`;
              ws.push({ name, tensor: wt });
            }
            out.push({ name: layer.name || `layer_${i}`, className: layer.getClassName ? layer.getClassName() : "Layer", weights: ws });
          }
          return out;
        }

        // Funktion für Nr. 8 (Word Algebra)
        async function findNearestNeighbor(queryVector) {
            if (!hasModel()) return { token: "<NO MODEL>", similarity: 0 };
            const embeddingMatrix = getEmbeddingMatrix(); // tf.Tensor2D (VocabSize x Dim)
            const tokens = Vocab.exportVocab().idToToken;

            if (!embeddingMatrix || embeddingMatrix.shape[0] === 0) return { token: "<EMPTY VOCAB>", similarity: 0 };

            let simScores = null;
            let maxSim = -Infinity;
            let nearestId = -1;

            try {
                // Query Vector muss (1 x Dim) sein. Squeeze, falls nötig
                const queryTensor = queryVector.reshape([1, -1]);

                // 1. Matrizen-Multiplikation für Dot-Products: (1 x Dim) * (Dim x VocabSize) = (1 x VocabSize)
                const embeddingMatrixT = embeddingMatrix.transpose(); // (Dim x VocabSize)
                const dotProducts = queryTensor.matMul(embeddingMatrixT).squeeze(); // 1D Tensor (VocabSize)

                // 2. Normen berechnen
                const embNorms = tf.norm(embeddingMatrix, 2, 1); // 1D Tensor (VocabSize)
                const queryNormScalar = tf.norm(queryTensor).dataSync()[0];

                // 3. Similarity = DotProducts / (queryNormScalar * embNorms)
                simScores = dotProducts.div(embNorms.mul(queryNormScalar));

                // 4. Index mit höchster Ähnlichkeit finden
                const maxIndexTensor = simScores.argMax();
                nearestId = maxIndexTensor.dataSync()[0];
                maxSim = simScores.dataSync()[nearestId];

                // Aufräumen von Zwischen-Tensoren
                dotProducts.dispose();
                embeddingMatrixT.dispose();
                embNorms.dispose();
                maxIndexTensor.dispose();
                queryTensor.dispose();
                
                return { token: tokens[nearestId] || "<UNK>", similarity: maxSim };

            } catch (e) {
                console.error("findNearestNeighbor error", e);
                if (simScores) simScores.dispose();
                return { token: "<ERROR>", similarity: 0 };
            }
        }
        return { createModel, hasModel, getEmbeddingMatrix, trainOnDataset, resetModel, readModelInternals, findNearestNeighbor };
      })();

      // Visualizations
      let lastPlotLayout = null;
      const Visual = (function() {
        
        function layoutTransparent(title, extras) {
            const base = { title, margin:{t:30}, paper_bgcolor:"rgba(0,0,0,0)", plot_bgcolor:"rgba(0,0,0,0)", font:{color:"#e6eef6"} };
            if (extras) Object.assign(base, extras);
            return base;
        }

        function extractEmbeddings(embVar) {
            return tf.tidy(() => embVar.arraySync());
        }

        async function projectPCA(points, dimensions) {
            if (!points || points.length === 0) return [];
            const d = points[0].length;
            const targetDim = Math.min(d, dimensions);

            let matrix = null;
            let mean = null;
            let centered = null;
            let principalComponents = null;
            let reduced = null;
            let res = [];
            
            try {
                matrix = tf.tensor2d(points);
                mean = matrix.mean(0);
                centered = matrix.sub(mean);
                
                // Use the custom SVD (via Eigen-decomposition)
                const { v } = svd(centered);
                principalComponents = v;
                
                // Keep only the first 'targetDim' components (columns)
                const componentsSubset = principalComponents.slice([0, 0], [d, targetDim]);
                
                // Project the data: Reduced = Centered * ComponentsSubset
                reduced = centered.matMul(componentsSubset);
                
                res = reduced.arraySync();
            } catch(e) {
                console.error("PCA failed", e);
                pushLog("error", "PCA failed: "+e.message);
            } finally {
                if (matrix) matrix.dispose(); 
                if (mean) mean.dispose(); 
                if (centered) centered.dispose(); 
                if (principalComponents) principalComponents.dispose(); 
                if (reduced) reduced.dispose(); 
            }
            return res;
        }
        
        function cosineSimilarity(vecA, vecB) { 
            let dotProduct = 0; let normA = 0; let normB = 0;
            const len = Math.min(vecA.length, vecB.length);
            for (let i = 0; i < len; i++) {
                dotProduct += vecA[i] * vecB[i];
                normA += vecA[i] * vecA[i];
                normB += vecB[i] * vecB[i];
            }
            if (normA === 0 || normB === 0) return 0;
            return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
        }

        function distanceMatrix(points, labels) { 
            const n = points.length;
            const z = Array.from({length:n}, (_,i) => Array.from({length:n}, (_,j) => {
                if (i===j) return 1;
                return cosineSimilarity(points[i], points[j]);
            }));
            return { x: labels, y: labels.slice(), z };
        } 
        
        function correlationMatrix(points) { 
            if (!points || points.length === 0) return { x: [], y: [], z: [[]] };
            const d = points[0].length;
            if (d < 2) return { x: [], y: [], z: [[]] };
            const x = Array.from({length:d},(_,i)=>`dim${i+1}`);
            
            const matrix = tf.tensor2d(points);
            const centered = matrix.sub(matrix.mean(0));
            const N = points.length;
            const cov = centered.transpose().matMul(centered).div(N-1 || 1);
            
            // Correlation Matrix: R_ij = C_ij / (sqrt(C_ii) * sqrt(C_jj))
            const stdDevs = tf.sqrt(cov.diag());
            const outerProductStdDevs = stdDevs.expandDims(1).matMul(stdDevs.expandDims(0));
            const correlationMatrixTensor = cov.div(outerProductStdDevs);
            
            const z = correlationMatrixTensor.arraySync();
            
            matrix.dispose(); centered.dispose(); cov.dispose(); stdDevs.dispose(); outerProductStdDevs.dispose(); correlationMatrixTensor.dispose();
            
            return { x, y: x.slice(), z };
        }

        async function visualizePoints(points, labels, dims, mode, liveEmbeddings) {
            try {
                const d = points[0] ? points[0].length : 0;
                if (d < 1) { Plotly.react("plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Embeddings: No data")); return; }

                // Nr. 1 & 2: Placeholder für t-SNE/UMAP/Dendrogramm
                const originalMode = mode;
                if (mode === "tsne" || mode === "dendrogram") {
                    pushLog("info", `Visualisierung ${mode.toUpperCase()} ist didaktischer Placeholder, nutzt PCA 2D.`);
                    mode = "pca"; 
                }
                
                if (dims < 4 && mode !== "parallel" && mode !== "pairwise" && mode !== "heatmap") {
                    // Main plot: 1D, 2D, 3D
                    if (dims === 1 || dims === 2) {
                        const data = [{ 
                            x: points.map(p=>p[0]), 
                            y: points.map(p=>p[1]||0), 
                            text: labels, 
                            mode: "markers+text", 
                            type:"scatter", 
                            marker:{size:8} 
                        }];
                        const lay = layoutTransparent(`${dims}D embeddings`);
                        Plotly.react("plot", data, lay); 
                        lastPlotLayout = lay;
                    } else { // 3D
                        const data = [{ 
                            x: points.map(p=>p[0]), 
                            y: points.map(p=>p[1]), 
                            z: points.map(p=>p[2]), 
                            text: labels, 
                            mode:"markers", 
                            type:"scatter3d", 
                            marker:{size:4} 
                        }];
                        const lay = layoutTransparent("3D embeddings", {scene:{bgcolor:"rgba(0,0,0,0)"}});
                        Plotly.react("plot", data, lay);
                        lastPlotLayout = lay;
                    }
                    Plotly.react("highdim-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("High-dim: Not needed"));
                } else {
                    // High-dim: render in #highdim-plot
                    const target = "highdim-plot";
                    Plotly.react("plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Embeddings (High-Dim View Active)"));

                    if (mode === "auto") { mode = (d > 6) ? "parallel" : "pca"; }
                    
                    let data = [];
                    let layout;

                    if (mode === "pairwise") { 
                        pushLog("info", "Pairwise Scatter Matrix ist ein Placeholder, zeigt Correlation Heatmap.");
                        mode = "heatmap"; // Fallback to a simpler visualization
                    }
                    
                    if (mode === "parallel") {
                        const dimensions = Array.from({length:d}, (_, i) => ({
                            range: [Math.min(...points.map(p=>p[i])), Math.max(...points.map(p=>p[i]))],
                            label: `Dim ${i+1}`,
                            values: points.map(p=>p[i])
                        }));
                        data.push({
                            type: 'parcoords',
                            line: { showscale: false, color: '#5eead4' },
                            dimensions
                        });
                        layout = layoutTransparent("High-Dim: Parallel Coordinates");
                        Plotly.react(target, data, layout);

                    } else if (mode === "heatmap") {
                        const corr = correlationMatrix(points);
                        data.push({ z:corr.z, x:corr.x, y:corr.y, type:"heatmap", showscale:true, name:"Correlation Heatmap", colorscale:"Viridis" });
                        layout = layoutTransparent("High-Dim: Feature Correlation Heatmap");
                        Plotly.react(target, data, layout);
                        
                    } else if (mode === "pca") {
                        const pca = await projectPCA(points, 2);
                        const used = pca[0] ? pca[0].length : 2;
                        const dimsLabels = Array.from({length:used},(_,i)=>`PC${i+1}`);
                        const scatter = { x: pca.map(r=>r[0]), y: pca.map(r=>r[1]), mode:"markers", type:"scatter", marker:{size:6}, text: labels, name:`PCA 2D (${dimsLabels[0]} vs ${dimsLabels[1]})` };
                        
                        const title = (originalMode === "tsne" || originalMode === "dendrogram") 
                            ? `High-Dim: ${originalMode.toUpperCase()} (using PCA 2D placeholder)`
                            : "High-Dim: PCA 2D";

                        Plotly.react(target, [scatter], layoutTransparent(title));
                    }
                }
                
                // Distances Heatmap (always updated)
                const dm = distanceMatrix(points, labels);
                const distLay = layoutTransparent("Cosine-Ähnlichkeits-Heatmap (Embeddings)");
                Plotly.react("dist-heatmap", [{ z:dm.z, x:dm.x, y:dm.y, type:"heatmap", colorscale:"RdBu", zmin:-1, zmax:1 }], distLay);
            } catch (err) { 
                console.error("visualizePoints", err); 
                pushLog("error", "visualizePoints: "+err.message); 
            }
        }
        
        function plotLoss(losses) {
            const x = losses.map((_, i) => i + 1);
            const data = [{ x, y: losses, mode: "lines+markers", type: "scatter", name: "Training Loss", line: { color: "var(--accent)" } }];
            const lay = layoutTransparent("Loss pro Epoche", { xaxis: { title: "Epoche" }, yaxis: { title: "Loss" }, height:150, margin:{t:25, b:35, l:40, r:10} });
            Plotly.react("loss-plot", data, lay);
        }
        
        // Nr. 7: Histogramm-Verteilung der Embedding-Dimensionen
        function plotDimensionHistograms(points, labels, maxDims = 8) {
            const d = points[0] ? points[0].length : 0;
            const actualDims = Math.min(d, maxDims);
            if (actualDims < 2) {
                Plotly.react("dim-hist-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Histogramm: Mind. 2 Dimensionen benötigt"));
                return;
            }
            
            const traces = [];
            const rows = Math.min(2, Math.ceil(actualDims / 4));
            const cols = Math.min(4, actualDims);
            const totalPlots = rows * cols;
            
            const subplots = Array.from({length: rows}, (_, r) => 
                Array.from({length: cols}, (_, c) => `xy${r*cols + c + 1}`)
            );

            for (let i = 0; i < actualDims; i++) {
                const dimData = points.map(p => p[i]);
                traces.push({
                    x: dimData,
                    type: 'histogram',
                    name: `Dim ${i+1}`,
                    xaxis: `x${i+1}`,
                    yaxis: `y${i+1}`,
                    marker: { color: i % 2 === 0 ? 'var(--accent)' : '#60a5fa' },
                    opacity: 0.7,
                    showlegend: false
                });
            }

            const lay = layoutTransparent(`Verteilung der Werte in Dimensionen 1 bis ${actualDims}`, {
                grid: { rows, columns: cols, subplots: subplots.map(r => r.filter((_, i) => i < cols)) },
                showlegend: false,
                height: 360,
                margin: {t:30, b:20, l:20, r:10}
            });

            for (let i = 1; i <= actualDims; i++) {
                lay[`xaxis${i}`] = { title: `Dim ${i}`, showgrid: false, zeroline: false, type: 'linear' };
                lay[`yaxis${i}`] = { title: "Häufigkeit", showgrid: false, zeroline: false };
            }

            Plotly.react("dim-hist-plot", traces, lay);
        }

        // Nr. 6: Token Profile / Radar Plot
        function plotTokenProfiles(tokens, embeddings, labels) {
            const ids = Vocab.encode(tokens);
            const validIds = ids.filter(id => id < embeddings.length);
            const validTokens = validIds.map(id => labels[id]);

            if (validTokens.length === 0) {
                Plotly.react("radar-plot", [], layoutTransparent("Token Profile: Keine gültigen Tokens gefunden"));
                return;
            }

            const d = embeddings[0] ? embeddings[0].length : 0;
            const dims = Array.from({length: d}, (_, i) => `Dim ${i+1}`);
            const data = [];

            // Normalize embedding values (Min/Max Scaling)
            let allValues = embeddings.flat();
            const maxVal = Math.max(...allValues);
            const minVal = Math.min(...allValues);

            validIds.forEach((id, index) => {
                const emb = embeddings[id];
                const normalizedEmb = emb.map(v => (v - minVal) / (maxVal - minVal) );

                data.push({
                    type: 'scatterpolar',
                    r: normalizedEmb.concat(normalizedEmb[0]), // Close the loop
                    theta: dims.concat(dims[0]),
                    fill: 'toself',
                    name: labels[id]
                });
            });

            const lay = layoutTransparent("Token-Profile (Normalisiert 0-1)", {
                polar: {
                    radialaxis: { visible: true, range: [0, 1], showline: false, tickfont:{size:10} },
                    angularaxis: { direction: 'clockwise', period: d, showline: false, tickfont:{size:10} }
                },
                showlegend: true,
                height: 250,
                margin: {t:20, b:20, l:20, r:20}
            });

            Plotly.react("radar-plot", data, lay);
        }

        // Nr. 9: Recurrence Plot
        function plotRecurrencePlot(text) {
            const tokens = liveTokenizer(text);
            if (tokens.length < 2) {
                Plotly.react("recurrence-plot", [], layoutTransparent("Recurrence Plot: Mind. 2 Tokens benötigt"));
                return;
            }
            const tokenIds = Vocab.encode(tokens);
            const emb = Trainer.getEmbeddingMatrix();
            
            if (!emb) {
                Plotly.react("recurrence-plot", [], layoutTransparent("Recurrence Plot: Modell nicht erstellt"));
                return;
            }

            tf.tidy(() => {
                const embeddings = emb.arraySync();
                const sequenceEmbeddings = tokenIds.map(id => embeddings[id]);
                const N = sequenceEmbeddings.length;
                const labels = tokens.slice(0, N);

                const similarityMatrix = Array.from({length:N}, (_, i) => 
                    Array.from({length:N}, (_, j) => 
                        cosineSimilarity(sequenceEmbeddings[i], sequenceEmbeddings[j])
                    )
                );

                const data = [{
                    z: similarityMatrix,
                    x: labels,
                    y: labels.slice(),
                    type: "heatmap",
                    colorscale: "Viridis",
                    zmin: -1,
                    zmax: 1
                }];

                const lay = layoutTransparent("Sequenzielle Ähnlichkeit (Cosine Sim.)", {
                    xaxis: { title: "Token Position (i)" },
                    yaxis: { title: "Token Position (j)", autorange: 'reversed' },
                    height: 250,
                    margin: {t:30, b:40, l:40, r:10}
                });

                Plotly.react("recurrence-plot", data, lay);
            });
        }

        return { extractEmbeddings, visualizePoints, projectPCA, plotLoss, cosineSimilarity, distanceMatrix, correlationMatrix, plotDimensionHistograms, plotTokenProfiles, plotRecurrencePlot };
      })();

      // MathML helper
      async function tensorToMathML(tensor) { 
        if (!tensor) return { mathml: "<math></math>", meta: "null" };
        let arr; 
        try { if (typeof tensor.arraySync === "function") arr = tensor.arraySync(); else arr = await tensor.array(); } catch(e) { try { arr = await tensor.array(); } catch(e2) { arr = null; } }
        const shape = tensor.shape ? tensor.shape.slice() : (Array.isArray(arr) ? (Array.isArray(arr[0]) ? [arr.length, arr[0].length] : [arr.length]) : []);
        const dtype = tensor.dtype || "float32";
        const size = tensor.size || (Array.isArray(arr) ? (function count(a){ if (!Array.isArray(a)) return 1; return a.reduce((s,x)=>s+count(x),0); })(arr) : 0);
        function escapeHtml(s) { return String(s).replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;"); }
        function rowToTR(r) { if (!Array.isArray(r)) return `<mtr><mtd><mn>${escapeHtml(String(r))}</mn></mtd></mtr>`; return `<mtr>${r.map(c=>`<mtd><mn>${escapeHtml(String(c))}</mn></mtd>`).join("")}</mtr>`; }
        let mtable = "";
        if (Array.isArray(arr) && Array.isArray(arr[0])) { 
            mtable = `<mtable>${arr.map(r=> Array.isArray(r) ? `<mtr>${r.map(c=>`<mtd><mn>${escapeHtml(String(c))}</mn></mtd>`).join("")}</mtr>` : `<mtr><mtd><mn>${escapeHtml(String(r))}</mn></mtd></mtr>`).join("")}</mtable>`; 
        } else if (Array.isArray(arr)) { 
            mtable = `<mtable><mtr>${arr.map(c=>`<mtd><mn>${escapeHtml(String(c))}</mn></mtd>`).join("")}</mtr></mtable>`; 
        } else { 
            mtable = `<mtable><mtr><mtd><mn>${escapeHtml(String(arr))}</mn></mtd></mtr></mtable>`; 
        }
        const mathml = `<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">${mtable}</math>`;
        const meta = `shape: [${shape.join(", ")}] | dtype: ${dtype} | size: ${size}`;
        return { mathml, meta, array: arr };
      }

      // Live Tokenizer & Vocab State
      let liveTokenizer = Tokenizers.buildTokenizer($("#tokenizer-type").value, { n:parseInt($("#ngram-n").value,10)||2, mode:$("#ngram-mode").value||"word" });

      // UI Init & Event Handlers
      function initUI() {
        const inputText = $("#input-text");
        const tokenizerType = $("#tokenizer-type");
        const ngramN = $("#ngram-n");
        const ngramMode = $("#ngram-mode");
        
        function refreshTokenizer() {
            const n = Math.max(1,parseInt(ngramN.value,10));
            const mode = ngramMode.value;
            liveTokenizer = Tokenizers.buildTokenizer(tokenizerType.value, { n, mode });
            if (inputText.value.trim().length > 0) { tokenizeAndPreview(false); }
        }
        tokenizerType.addEventListener("change", refreshTokenizer);
        ngramN.addEventListener("input", refreshTokenizer);
        ngramMode.addEventListener("change", refreshTokenizer);
        inputText.addEventListener("input", () => tokenizeAndPreview(false));
        $("#btn-tokenize").addEventListener("click", ()=> tokenizeAndPreview(true));
        $("#btn-build-vocab").addEventListener("click", ()=>{ 
            const toks = tokenizeAndPreview(true); 
            Vocab.buildFromTokens(toks); 
            $("#stat-vocab-size") && ($("#stat-vocab-size").textContent = Vocab.size()); 
            pushLog("info", "vocab built "+Vocab.size()); 
            const embeddingDim = Math.max(1, parseInt($("#embedding-dim").value,10) || 8); 
            $("#status-emb-size").textContent = `${Vocab.size()}×${embeddingDim}`; 
        });
        $("#btn-clear").addEventListener("click", ()=> { 
            inputText.value=""; 
            tokenizeAndPreview(true); 
            Vocab.reset(); 
            $("#status-emb-size").textContent="0"; 
        });
        
        $("#btn-create-model").addEventListener("click", async () => { 
            try { 
                if (Vocab.size() === 0) { 
                    $("#btn-build-vocab").click(); 
                    await new Promise(r=>setTimeout(r,50)); 
                } 
                const vocabSize = Math.max(1, Vocab.size() || 4); 
                const embeddingDim = Math.max(1, parseInt($("#embedding-dim").value,10) || 8); 
                const optimizer = $("#optimizer-type").value; 
                const lr = parseFloat($("#learning-rate").value) || 0.01; 
                const m = await Trainer.createModel({ inputDim: vocabSize, outputDim: embeddingDim, optimizer, learningRate: lr }); 
                $("#status-emb-size").textContent = `${vocabSize}×${embeddingDim}`; 
                pushLog("info","created model"); 
                updateAllVisualizations(); 
                updateWordAlgebra(); // Nr. 8: Initialisiere Word Algebra
            } catch (err) { 
                pushLog("error","create model failed: "+err.message); 
            } 
        });
        
        $("#btn-train").addEventListener("click", async () => { 
            try { 
                if (Vocab.size() === 0) { pushLog("warn", "vocab is empty, building vocab first..."); $("#btn-build-vocab").click(); await new Promise(r=>setTimeout(r,50)); }
                if (!Trainer.hasModel()) { pushLog("warn", "model not created, creating model first..."); $("#btn-create-model").click(); await new Promise(r=>setTimeout(r,50)); }
                const vocabSize = Vocab.size();
                const encodedTokens = Vocab.encode(liveTokenizer($("#input-text").value));
                pushLog("info", `training on ${encodedTokens.length} tokens for ${$("#epochs").value} epochs...`);
                await Trainer.trainOnDataset(encodedTokens, { batchSize:parseInt($("#batch-size").value,10), epochs:parseInt($("#epochs").value,10) }, {
                    onEpochEnd: async () => { updateAllVisualizations(); } // Re-plot on epoch end
                });
                pushLog("info", "training finished");
            } catch (err) { pushLog("error", "training failed: "+err.message); }
        });

        $("#btn-export-emb").addEventListener("click", async () => {
            const emb = Trainer.getEmbeddingMatrix();
            if (!emb) { pushLog("warn", "No embedding matrix to export."); return; }
            const raw = await emb.array();
            const labels = Vocab.exportVocab().idToToken;
            const data = JSON.stringify({ metadata: labels, embeddings: raw }, null, 2);
            const blob = new Blob([data], { type: 'application/json' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'embeddings_export.json';
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
            pushLog("info", "Embeddings exported to embeddings_export.json");
        });
        $("#btn-reset").addEventListener("click", () => { Trainer.resetModel(); updateAllVisualizations(); pushLog("info", "Model and plots reset."); });
        $("#visual-dims").addEventListener("change", updateAllVisualizations);
        $("#visual-mode").addEventListener("change", updateAllVisualizations);
        $("#auto-highdim-toggle").addEventListener("change", updateAllVisualizations);

        $("#live-text").addEventListener("input", () => liveMode && probeNow(true));
        $("#btn-live-toggle").addEventListener("click", () => { liveMode = !liveMode; $("#btn-live-toggle").textContent = liveMode ? "Live (on)" : "Live (off)"; if (liveMode) { probeNow(true); pushLog("info", "live mode activated"); } else { pushLog("info", "live mode deactivated"); } });
        $("#btn-probe-now").addEventListener("click", () => probeNow(true));
        $("#btn-show-emb-math").addEventListener("click", async () => { try { await renderEmbeddingInternals(); } catch (e) { console.error(e); } });
        $("#btn-show-model-internals").addEventListener("click", async () => { try { await renderModelInternals(); } catch (e) { console.error(e); } });
        $("#run-tests").addEventListener("click", runTests);
        
        // Nr. 6: Plot Radar
        $("#btn-plot-radar").addEventListener("click", () => {
            const tokens = $("#radar-tokens").value.split(",").map(t => t.trim()).filter(Boolean);
            const embVar = Trainer.getEmbeddingMatrix();
            if (embVar) {
                const raw = embVar.arraySync();
                const labels = Vocab.exportVocab().idToToken;
                Visual.plotTokenProfiles(tokens, raw, labels);
            } else {
                pushLog("warn", "Cannot plot radar: Model not created.");
            }
        });

        // Nr. 9: Plot Recurrence
        $("#btn-plot-recurrence").addEventListener("click", () => {
            const text = $("#input-text").value;
            Visual.plotRecurrencePlot(text);
        });

        // Nr. 8: Word Algebra
        $("#algebra-token-a").addEventListener("input", updateWordAlgebra);
        $("#algebra-token-b").addEventListener("input", updateWordAlgebra);
        $("#algebra-slider").addEventListener("input", updateWordAlgebra);

      }
      
      // Nr. 8: Word Algebra Logic
      async function updateWordAlgebra() {
            const tokenA = $("#algebra-token-a").value.trim();
            const tokenB = $("#algebra-token-b").value.trim();
            const alpha = parseFloat($("#algebra-slider").value);
            $("#algebra-alpha").textContent = alpha.toFixed(2);

            if (!Trainer.hasModel() || Vocab.size() < 2) {
                $("#algebra-result-token").textContent = "Modell oder Vokabular fehlt.";
                return;
            }
            
            const ids = Vocab.encode([tokenA, tokenB]);
            const exists = ids.every(id => id < Vocab.size());
            if (ids.length < 2 || !exists) {
                $("#algebra-result-token").textContent = "Bitte zwei unterschiedliche, gültige Tokens eingeben.";
                return;
            }


            const embMatrix = Trainer.getEmbeddingMatrix();
            const A = embMatrix.slice([ids[0], 0], [1, embMatrix.shape[1]]);
            const B = embMatrix.slice([ids[1], 0], [1, embMatrix.shape[1]]);

            // Interpolation: I = A + alpha * (B - A)
            const B_minus_A = B.sub(A);
            const diff_scaled = B_minus_A.mul(alpha);
            const I = A.add(diff_scaled);

            const result = await Trainer.findNearestNeighbor(I);
            
            $("#algebra-result-token").innerHTML = `
                **${result.token}** (Sim: ${result.similarity.toFixed(4)})
                <br><small class="muted">Interpolation: ${tokenA} + ${alpha.toFixed(2)} * (${tokenB} - ${tokenA})</small>
            `;
      }


      async function probeNow(silent=false) { 
        if (!Trainer.hasModel()) { if (!silent) pushLog("warn", "Model not created yet."); return; }
        const text = $("#live-text").value.trim();
        if (text.length === 0) { Plotly.react("live-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Live Probe: No text")); return; }
        
        const tokens = liveTokenizer(text);
        const encoded = Vocab.encode(tokens);
        if (encoded.length === 0) { Plotly.react("live-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Live Probe: No tokens")); return; }
        
        const vocabSize = Vocab.size();
        const outputProbabilities = tf.tidy(() => {
            const xs = tf.tensor1d([encoded[0]], "int32").expandDims(1);
            const output = Trainer.model.predict(xs);
            return output.squeeze().arraySync();
        });
        
        const vocabLabels = Vocab.exportVocab().idToToken;
        
        // 1. Bar Plot for Output Probabilities (Prediction)
        const topK = 10;
        const sortedProbs = outputProbabilities.map((prob, index) => ({ prob, token: vocabLabels[index] }))
            .sort((a, b) => b.prob - a.prob)
            .slice(0, Math.min(topK, vocabLabels.length));
        
        const data = [{
            x: sortedProbs.map(p => p.prob),
            y: sortedProbs.map(p => p.token),
            type: 'bar',
            orientation: 'h',
            marker: { color: 'var(--accent)' }
        }];
        const layout = layoutTransparent(`Next Token Prediction (for '${tokens[0]}')`, { 
            yaxis: { autorange: 'reversed' }, 
            xaxis: { range: [0, 1] }, 
            height:220, 
            margin:{t:30, b:35, l:80, r:10} 
        });
        Plotly.react("live-plot", data, layout);

        if (!silent) pushLog("info", `Probed live text: next token for '${tokens[0]}' predicted.`);
      }

      async function updateAllVisualizations() {
        const embVar = Trainer.getEmbeddingMatrix();
        if (embVar) {
            const raw = await Visual.extractEmbeddings(embVar);
            const labels = Vocab.exportVocab().idToToken;
            const dimsReq = parseInt($("#visual-dims").value,10);
            const mode = $("#visual-mode").value;
            const autoHighDim = $("#auto-highdim-toggle").checked;
            
            // Auto switch to high-dim view if >3 dimensions are chosen and auto is on
            const visualModeOverride = (dimsReq > 3 && autoHighDim) ? "auto" : mode;

            await Visual.visualizePoints(raw, labels, dimsReq, visualModeOverride, null);
            await Visual.plotDimensionHistograms(raw, labels); // Nr. 7
            await renderEmbeddingInternals(true); 
            if (liveMode) await probeNow(true);
            updateWordAlgebra(); // Nr. 8 aktualisieren
        } else {
            Plotly.react("plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Embeddings"));
            Plotly.react("highdim-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("High-dim"));
            Plotly.react("dist-heatmap", [{ x:[], y:[], z:[], type:"heatmap" }], layoutTransparent("Cosine-Ähnlichkeits-Heatmap"));
            Plotly.react("dim-hist-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Histogramm-Verteilung")); // Nr. 7 aufräumen
            Plotly.react("radar-plot", [], layoutTransparent("Token Profile")); // Nr. 6 aufräumen
            Plotly.react("recurrence-plot", [], layoutTransparent("Recurrence Plot")); // Nr. 9 aufräumen
            $("#algebra-result-token").textContent = "Modell fehlt."; // Nr. 8 aufräumen
        }
      } 
      
      async function renderEmbeddingInternals(silent=false) {
        const embVar = Trainer.getEmbeddingMatrix();
        if (!embVar) { if (!silent) pushLog("warn", "No embedding matrix available."); return; }
        
        const { mathml, meta, array: arr } = await tensorToMathML(embVar);
        
        $("#tensor-meta").textContent = `Embedding Matrix: ${meta}`;
        $("#tensor-mathml-render").innerHTML = `<pre class="mathml">${mathml}</pre>`;
        
        if (arr && arr.length > 0) {
            const z = arr;
            const x = Array.from({length:z[0].length}, (_,i)=>`dim${i+1}`);
            const y = Vocab.exportVocab().idToToken;
            
            const data = [{
                z, x, y, type: 'heatmap',
                colorscale: 'Viridis',
                colorbar: { title: 'Value' }
            }];
            const layout = layoutTransparent("Embedding Matrix Heatmap");
            Plotly.react("tensor-heatmap", data, layout);
        } else {
            Plotly.react("tensor-heatmap", [], layoutTransparent("Embedding Matrix Heatmap"));
        }
      }
      
      async function renderModelInternals() {
        const internals = await Trainer.readModelInternals();
        if (!internals || internals.length === 0) { pushLog("warn", "Model internals not available."); return; }
        
        let html = "<h3>Model Weights</h3>";
        let heatmapData = [];
        let plotLayout = layoutTransparent("Model Internals Heatmaps", { grid: { rows: internals.length-1, columns: 1, pattern: 'independent' }, height: (internals.length-1)*200+50 });
        
        for (const layer of internals) {
            html += `<h4>${layer.name} (${layer.className})</h4>`;
            for (const weight of layer.weights) {
                const { mathml, meta, array: arr } = await tensorToMathML(weight.tensor);
                html += `<h5>${weight.name} (W/B) - ${meta}</h5>`;
                html += `<pre class="mathml">${mathml}</pre>`;

                if (arr && Array.isArray(arr[0])) {
                    // Create heatmap for 2D tensors (Kernels/Weights)
                    const z = arr;
                    const x = Array.from({length:z[0].length}, (_,i)=>`output_unit_${i+1}`);
                    const y = Array.from({length:z.length}, (_,i)=>`input_unit_${i+1}`);

                    heatmapData.push({
                        z, x, y, type: 'heatmap',
                        colorscale: 'RdBu',
                        colorbar: { title: 'Weight' },
                        title: `${layer.name}: ${weight.name}`,
                        xaxis: `x${heatmapData.length+1}`,
                        yaxis: `y${heatmapData.length+1}`
                    });
                }
            }
        }
        
        // Remove embedding layer from heatmap view as it is shown separately
        const heatmapTraces = heatmapData.filter(d => !d.title.includes("embedding"));

        $("#model-internals-render").innerHTML = html;
        if (heatmapTraces.length > 0) {
             // Redo layout for the remaining heatmaps
             const numTraces = heatmapTraces.length;
             plotLayout = layoutTransparent("Model Internals Heatmaps (Kernels & Biases)", { 
                 grid: { rows: numTraces, columns: 1, pattern: 'independent' }, 
                 height: numTraces*300+50 
             });
             heatmapTraces.forEach((trace, i) => {
                 plotLayout[`xaxis${i+1}`] = { title: 'Output Dimension/Unit' };
                 plotLayout[`yaxis${i+1}`] = { title: 'Input Dimension/Unit/Bias' };
                 trace.xaxis = `x${i+1}`;
                 trace.yaxis = `y${i+1}`;
             });
             Plotly.react("tensor-heatmap", heatmapTraces, plotLayout);
        } else {
            Plotly.react("tensor-heatmap", [], layoutTransparent("Model Internals Heatmaps (Only 1D tensors found)"));
        }
        pushLog("info", "rendered model internals");
      }
      
      function tokenizeAndPreview(force) {
        const text = $("#input-text").value;
        const tokens = liveTokenizer(text);
        
        if (!force && tokens.length < 50 && tokens.length === (parseInt($("#stat-token-count").textContent,10)||0)) return tokens;

        const uniqueTokens = Array.from(new Set(tokens.filter(Boolean)));
        
        $("#token-preview").innerHTML = tokens.slice(0,50).map(t => `<span class="token-pill">${escapeHtml(t)}</span>`).join("");
        if (tokens.length > 50) $("#token-preview").innerHTML += `... und ${tokens.length-50} weitere.`;

        $("#stat-token-count").textContent = tokens.length;
        $("#stat-unique-count").textContent = uniqueTokens.length;
        
        return tokens;
      }
      
      async function runTests() {
        console.group("Running Tests");
        pushLog("info", "Starting integration tests...");

        try {
            // 1. Tokenizer Test
            $("#tokenizer-type").value = "whitespace";
            $("#input-text").value = "eins zwei drei";
            tokenizeAndPreview(true);
            const tokens = ["eins", "zwei", "drei"];
            const currentTokens = liveTokenizer($("#input-text").value);
            if (currentTokens.join(" ") !== tokens.join(" ")) throw new Error("Tokenizer test failed.");
            pushLog("info", "Tokenizer test successful.");

            // 2. Vocab Test
            Vocab.buildFromTokens(currentTokens);
            if (Vocab.size() !== 3) throw new Error("Vocab size test failed.");
            const encoded = Vocab.encode(tokens);
            if (encoded.length !== 3 || encoded.some(id => typeof id !== 'number')) throw new Error("Vocab encode test failed.");
            pushLog("info", "Vocab test successful.");

            // 3. Model Creation Test
            $("#embedding-dim").value = 4;
            $("#learning-rate").value = 0.1;
            await $("#btn-create-model").click();
            if (!Trainer.hasModel()) throw new Error("Model creation test failed.");
            const embMatrix = Trainer.getEmbeddingMatrix();
            if (!embMatrix || embMatrix.shape[0] !== 3 || embMatrix.shape[1] !== 4) throw new Error("Embedding matrix shape test failed.");
            pushLog("info", "Model creation test successful (3x4).");
            
            // 4. Training Step Test
            const initialLosses = epochLosses.length;
            await Trainer.trainOnDataset(encoded, { batchSize:3, epochs:1 }, null);
            if (epochLosses.length !== initialLosses + 1) throw new Error("Training step test failed (loss not recorded).");
            pushLog("info", "Training step test successful.");
            
            // 5. Visualization Test (simple check)
            await updateAllVisualizations();
            if (!$("#plot").querySelector('svg')) throw new Error("Visualization test failed (Plotly not rendered).");
            pushLog("info", "Visualization test successful.");
            
            // 6. Word Algebra Test (Nr. 8)
            $("#algebra-token-a").value = "eins";
            $("#algebra-token-b").value = "zwei";
            $("#algebra-slider").value = 0.5;
            await updateWordAlgebra();
            const algebraResult = $("#algebra-result-token").textContent;
            if (!algebraResult || algebraResult.includes("fehlt")) throw new Error("Word Algebra test failed.");
            pushLog("info", "Word Algebra test successful.");

            pushLog("info", "All integration tests passed successfully.");

        } catch (e) {
            pushLog("error", "Integration tests FAILED: " + e.message);
        } finally {
            Trainer.resetModel(); // Cleanup
            console.groupEnd();
        }
      }


      // startup
      function layoutTransparent(title, extras) {
          const base = { title, margin:{t:30}, paper_bgcolor:"rgba(0,0,0,0)", plot_bgcolor:"rgba(0,0,0,0)", font:{color:"#e6eef6"} };
          if (extras) Object.assign(base, extras);
          return base;
      }
      
      (function main() {
        try {
          initUI();
          tokenizeAndPreview(false);
          
          Plotly.newPlot("plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Embeddings"));
          Plotly.newPlot("highdim-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("High-dim"));
          Plotly.newPlot("live-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Live probe"));
          Plotly.newPlot("dist-heatmap", [{ x:[], y:[], z:[], type:"heatmap" }], layoutTransparent("Cosine-Ähnlichkeits-Heatmap"));
          
          // Initialisiere die neuen Plots (Nr. 6, 7, 9)
          Plotly.newPlot("radar-plot", [], layoutTransparent("Token Profile")); 
          Plotly.newPlot("recurrence-plot", [], layoutTransparent("Recurrence Plot"));
          Plotly.newPlot("dim-hist-plot", [{ x:[], y:[], mode:"markers" }], layoutTransparent("Histogramm-Verteilung"));
          
          Visual.plotLoss(epochLosses); 
          
          // Setze initiale Eingabetexte und starte Tokenizer/Vocab
          $("#input-text").value = "das ist ein test. test, test. das ist noch ein test.";
          $("#tokenizer-type").value = "whitespace"; 
          tokenizeAndPreview(true);
          $("#btn-build-vocab").click(); 
          updateWordAlgebra(); // Nr. 8: Initialisiere mit Default-Werten
          
          pushLog("info","App ready");
        } catch (e) { console.error("main init error", e); pushLog("error","App failed to start: "+e.message); }
      })();

    })();
  </script>
</body>
</html>
